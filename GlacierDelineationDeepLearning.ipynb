{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GlacierDelineationDeepLearning.ipynb","version":"0.3.2","provenance":[{"file_id":"1r0IfUC0PVgF-yGKAdE6l0EC71PTWjglS","timestamp":1561983640472}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dzqG3bhaLGU7","colab_type":"text"},"source":["This is a code to segment a Landsat ETM+ image in to glacier and non glacier binary image. The segmentation is performed using the U-Net convolutional Neural Network model trained using the Pytorch Library. \n","Part of the code is based on the following implementation: Deep networks for Earth Observation (https://github.com/nshaud/DeepNetsForEO). I would like to hereby acknowledge that the above mentioned respository has helped me in getting starting with Deep Learning for remote sensing image segmentation using Pytorch.\n","\n","In order to run this code, a Google account is required. The Landsat ETM+ image is exported from the EarthEngine to the Drive and then feeded in to the neural network for prediction. The exporting of the image to the Google Drive may take 5-10 minutes.\n","\n","The False color original image as well as the predicted mask is then displayed on the map. \n","\n","The user needs to auhtorize access to the Google Drive below\n"]},{"cell_type":"code","metadata":{"id":"u7r6sheJmYdl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"86e9f9a9-1a7b-4a59-9c58-36b01ee53a36","executionInfo":{"status":"ok","timestamp":1562844189062,"user_tz":-300,"elapsed":26307,"user":{"displayName":"Sajid Ghuffar","photoUrl":"","userId":"13181957101889559783"}}},"source":["!pip install pyproj\n","import torch\n","print(torch.__version__)\n","import torch.nn.functional as F\n","from skimage import io\n","from glob import glob\n","import folium\n","import random\n","import itertools\n","import gdal\n","import matplotlib.pyplot as plt\n","import os.path\n","from torch.autograd import Variable\n","import numpy as np\n","import os\n","import torch.nn as nn\n","from torchvision import transforms  \n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.optim.lr_scheduler\n","import torch.nn.init\n","from tqdm import tqdm\n","import numpy as np\n","from pyproj import Proj, transform\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')\n","    \n","use_cuda = True\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyproj in /usr/local/lib/python3.6/dist-packages (2.2.1)\n","1.1.0\n","CUDA is available!  Training on GPU ...\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a5pRKw-wawGA","colab_type":"text"},"source":["As we are using Earth Engine libraries to read and export the Landsat image, one would need to authorize access for Earth Engine. Copy paste the code below for authorization. "]},{"cell_type":"code","metadata":{"id":"O75RRJRqmk-M","colab_type":"code","outputId":"91987c8a-82b3-42c5-c426-04d3101f3718","executionInfo":{"status":"ok","timestamp":1562844181429,"user_tz":-300,"elapsed":24845,"user":{"displayName":"Sajid Ghuffar","photoUrl":"","userId":"13181957101889559783"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["!pip install earthengine-api  # install the Earth Engine API\n","!earthengine authenticate\n","import ee\n","ee.Initialize()\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: earthengine-api in /usr/local/lib/python3.6/dist-packages (0.1.182)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (0.0.3)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (0.11.3)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.4.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.12.0)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.7.9)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (3.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (0.2.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (4.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->earthengine-api) (3.0.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.4.5)\n","Running command using Cloud API.  Set --no-use_cloud_api to go back to using the API\n","Opening the following address in a web browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code\n","\n","Please authorize access to your Earth Engine account, and paste the generated code below. If the web browser does not start, please manually browse the URL above.\n","\n","Please enter authorization code: 4/gwE75tyU__MGi8_ULVvbI77G7eIz1rB4w4se8nhZnw5yiMUxl9r0WY8\n","\n","Successfully saved authorization token.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j1niJ1mguhZ8","colab_type":"code","outputId":"a5faa69c-8e6d-4d9d-f5d5-748ccc9beace","executionInfo":{"status":"ok","timestamp":1562844242336,"user_tz":-300,"elapsed":4143,"user":{"displayName":"Sajid Ghuffar","photoUrl":"","userId":"13181957101889559783"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\n","EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n","bands=['B1','B2','B3','B4','B5','B6','B7','B8','B9']\n","image=ee.Image('LANDSAT/LE07/C01/T1/LE07_150035_19990916')\n","projI=image.select('B1').projection().getInfo()#.transform()\n","crs=projI['crs']\n","outProj=Proj(init='epsg:4326')\n","inProj=Proj(init=crs)\n","kk=projI['transform']\n","print(projI)\n","lon1, lat1=transform(inProj,outProj,kk[2],kk[5])\n","lon2, lat2=transform(inProj,outProj,kk[2]+5000*30,kk[5]-5000*30) \n","\n","bounds = [lon1, lat2, lon2, lat1] ## sample land / sea bounds\n","print(bounds,lon1,lat1)\n","area = ee.Geometry.Rectangle(bounds)\n","image_clip = image.clip(area)\n","info = image_clip.getInfo()\n","#print(info)\n","stats = image.reduceRegion(reducer=ee.Reducer.toList(),geometry=area,scale=15,bestEffort=True)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["{'type': 'Projection', 'crs': 'EPSG:32643', 'transform': [30.0, 0.0, 262185.0, 0.0, -30.0, 4098315.0]}\n","[72.32743957658587, 35.13732759278308, 74.69467310722304, 37.00093063577626] 72.32743957658587 37.00093063577626\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MxdeXXEfrs17","colab_type":"code","outputId":"1440b79a-3da5-41a6-8e18-9d609966251e","executionInfo":{"status":"ok","timestamp":1562846028732,"user_tz":-300,"elapsed":1659228,"user":{"displayName":"Sajid Ghuffar","photoUrl":"","userId":"13181957101889559783"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["imageL7=ee.Image('LANDSAT/LE07/C01/T1/LE07_150035_19990916')\n","imageL7=imageL7.select(['B1','B2','B3','B4','B5','B6_VCID_2','B7','B8'])\n","geometry = ([lon1, lat1],[lon1,lat2],[lon2,lat2],[lon2 ,lat1])\n","#config={'folder':'DeepLearning', 'region':geometry ,'scale': 30, 'fileFormat': 'GeoTIFF', 'maxPixels':'1e10'}\n","config= {\n","    'description':'Landsat07image',    \n","    'region':geometry ,\n","    'scale': 15, \n","    'fileFormat': 'GeoTIFF',\n","    'maxPixels':'1e12'\n","}\n","\n","exp=ee.batch.Export.image.toDrive(imageL7,**config);\n","#exp=ee.batch.Export.image.toDrive(**config);\n","\n","exp.start()\n","print(exp.status())\n","print(ee.batch.Task.list())\n","import time\n","while exp.active():\n","  print('Transferring Data to Drive..................')\n","  time.sleep(30)\n","print('Done with the Export to the Drive')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{'id': 'S2N3FQOZNRTX6RGWO3YUZC7N', 'state': 'READY', 'creation_timestamp_ms': 1562844369633, 'update_timestamp_ms': 1562844369633, 'description': 'Landsat07image', 'task_type': 'EXPORT_IMAGE'}\n","[<Task EXPORT_IMAGE: Landsat07image (READY)>, <Task EXPORT_IMAGE: Landsat07image (COMPLETED)>, <Task EXPORT_IMAGE: Landsat07image (COMPLETED)>, <Task EXPORT_IMAGE: Landsat07image (COMPLETED)>, <Task EXPORT_IMAGE: Landsat07image (COMPLETED)>, <Task EXPORT_IMAGE: KKKKK (COMPLETED)>, <Task EXPORT_IMAGE: KKKKK (COMPLETED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (COMPLETED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: KKKKK (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: imagefdfd (FAILED)>, <Task EXPORT_IMAGE: image (FAILED)>, <Task EXPORT_IMAGE: image (FAILED)>, <Task EXPORT_IMAGE: image (FAILED)>, <Task EXPORT_IMAGE: image (FAILED)>, <Task EXPORT_IMAGE: image (FAILED)>, <Task EXPORT_IMAGE: exportExample (FAILED)>, <Task EXPORT_IMAGE: exportExample (FAILED)>, <Task EXPORT_IMAGE: exportExample (FAILED)>, <Task EXPORT_IMAGE: exportExample (FAILED)>, <Task EXPORT_IMAGE: exportExample (FAILED)>, <Task EXPORT_IMAGE: myExportImageTask (FAILED)>, <Task EXPORT_IMAGE: imageToCOGeoTiffExample11111 (FAILED)>, <Task EXPORT_IMAGE: imageToCOGeoTiffExample11111 (FAILED)>, <Task EXPORT_IMAGE: imageToCOGeoTiffExample11111 (FAILED)>, <Task EXPORT_IMAGE: imageToCOGeoTiffExample (FAILED)>]\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Transferring Data to Drive..................\n","Done with the Export to the Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PJr39RUXbnMw","colab_type":"code","colab":{}},"source":["class ConvBnRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n","        super(ConvBnRelu, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n","        #self.bn = nn.BatchNorm2d(in_channels)\n","        self.bn = nn.GroupNorm(in_channels,in_channels)\n","       # self.relu = nn.ReLU()\n","        self.relu = nn.LeakyReLU()\n","\n","    def forward(self, x): \n","        x = self.bn(x)\n","        x = self.conv(x)       \n","        x = self.relu(x)        \n","        return x\n","\n","\n","\n","class StackEncoder(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(StackEncoder, self).__init__()\n","        self.convr1 = ConvBnRelu(in_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n","        self.convr2 = ConvBnRelu(out_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n","        self.maxPool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n","\n","    def forward(self, x):\n","        x = self.convr1(x)\n","        x = self.convr2(x)\n","        \n","        x_trace = x\n","        x = self.maxPool(x)\n","        return x, x_trace\n","\n","\n","class StackDecoder(nn.Module):\n","    def __init__(self, in_channels1,in_channels2, out_channels):\n","        super(StackDecoder, self).__init__()\n","\n","        #self.upSample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n","        self.upSample = nn.ConvTranspose2d(in_channels1,in_channels1, (2,2), stride=2)\n","        \n","        self.convr1 = ConvBnRelu(in_channels1+in_channels2, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n","        self.convr2 = ConvBnRelu(out_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n","    def _crop_concat(self, upsampled, bypass):\n","        return torch.cat((upsampled, bypass), 1)\n","\n","    def forward(self, x, down_tensor):\n","        x = self.upSample(x)\n","        x = self._crop_concat(x, down_tensor)\n","        x = self.convr1(x)    \n","        x = self.convr2(x)\n","       # x = self.convr3(x)\n","        return x\n","\n","\n","class UNetOriginal(nn.Module):\n","    @staticmethod\n","    def weight_init(m):\n","        if isinstance(m, nn.Conv2d):\n","            torch.nn.init.kaiming_normal(m.weight.data)\n","\n","    def __init__(self, in_shape):\n","        super(UNetOriginal, self).__init__()\n","        channels, height, width = in_shape\n","\n","        self.down1 = StackEncoder(channels,16)\n","        self.down2 = StackEncoder(16, 16)\n","        self.down3 = StackEncoder(16, 32)\n","        self.down4 = StackEncoder(32,32)\n","\n","        self.center = nn.Sequential(\n","            ConvBnRelu(32, 32, kernel_size=(3, 3), stride=1, padding=1),\n","            ConvBnRelu(32, 32, kernel_size=(3, 3), stride=1, padding=1)\n","        )\n","\n","        self.up1 = StackDecoder(in_channels1=32,in_channels2=32, out_channels=32)\n","        self.up2 = StackDecoder(in_channels1=32,in_channels2=32, out_channels=32)\n","        self.up3 = StackDecoder(in_channels1=32,in_channels2=16, out_channels=16)\n","        self.up4 = StackDecoder(in_channels1=16,in_channels2=16, out_channels=16)\n","\n","        self.output_seg_map = nn.Conv2d(16, 2, kernel_size=(1, 1), padding=0, stride=1)\n","\n","    def forward(self, x):\n","        x, x_trace1 = self.down1(x)  \n","        x, x_trace2 = self.down2(x)\n","        x, x_trace3 = self.down3(x)\n","        x, x_trace4 = self.down4(x)\n","\n","        x = self.center(x)\n","\n","        x = self.up1(x, x_trace4)\n","        x = self.up2(x, x_trace3)\n","        x = self.up3(x, x_trace2)\n","        x = self.up4(x, x_trace1)\n","\n","        out = self.output_seg_map(x)\n","        out = torch.squeeze(out, dim=1)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"93g7du4TEW2m","colab_type":"code","outputId":"f924c6bf-3bce-4599-9e2c-73f2785071dc","executionInfo":{"status":"ok","timestamp":1562662361343,"user_tz":-300,"elapsed":2380,"user":{"displayName":"Sajid Ghuffar","photoUrl":"","userId":"13181957101889559783"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","WINDOW_SIZE = (512 ,512) # Patch size\n","STRIDE = 32 # Stride for testing\n","IN_CHANNELS =8  # Use 8 bands of ETM+\n","BATCH_SIZE = 1  # Number of samples in a mini-batch\n","\n","LABELS = [\"Backgr\",\"Glaciers\"] # Label names\n","N_CLASSES = len(LABELS) # Number of classes\n","WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n","CACHE = True # Store the dataset in-memory\n","\n","\n","\n","def CrossEntropy2d(input, target, weight=None, size_average=True):\n","    \"\"\" 2D version of the cross entropy loss \"\"\"\n","    dim = input.dim()\n","    if dim == 2:\n","        return F.cross_entropy(input, target, weight, size_average)\n","    elif dim == 4:\n","        output = input.view(input.size(0),input.size(1), -1)\n","        output = torch.transpose(output,1,2).contiguous()\n","        output = output.view(-1,output.size(2))\n","        target = target.view(-1)\n","        return F.cross_entropy(output, target,weight, size_average)\n","    else:\n","        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n","\n","def accuracy(input, target):\n","    return 100 * float(np.count_nonzero(input == target)) / target.size\n","\n","def sliding_window(top, step=10, window_size=(20,20)):\n","    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n","    for x in range(0, top.shape[0], step):\n","        if x + window_size[0] > top.shape[0]:\n","            x = top.shape[0] - window_size[0]\n","        for y in range(0, top.shape[1], step):\n","            if y + window_size[1] > top.shape[1]:\n","                y = top.shape[1] - window_size[1]\n","            yield x, y, window_size[0], window_size[1]\n","            \n","def count_sliding_window(top, step=10, window_size=(20,20)):\n","    \"\"\" Count the number of windows in an image \"\"\"\n","    c = 0\n","    for x in range(0, top.shape[0], step):\n","        if x + window_size[0] > top.shape[0]:\n","            x = top.shape[0] - window_size[0]\n","        for y in range(0, top.shape[1], step):\n","            if y + window_size[1] > top.shape[1]:\n","                y = top.shape[1] - window_size[1]\n","            c += 1\n","    return c\n","     \n","\n","def testOnly(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n","     \n","    #test_files  = glob(test_Folder)\n","    test_ids = list(range(1,len(test_files)+1))\n","    for k in range(len(test_ids)):\n","        \n","           \n","        test_images = (1 / 255 * np.asarray(io.imread(test_files[int(test_ids[k])-1]), dtype='float32') )\n","        \n","        all_preds = []\n","\n","        net.eval()\n","   \n","    \n","        img=test_images\n","        pred = np.zeros((img.shape[0],img.shape[1],N_CLASSES),)\n","        gt = np.zeros((img.shape[0],img.shape[1]))\n","        stride=256\n","        total = count_sliding_window(gt, step=stride, window_size=window_size) // batch_size\n","        for i, coords in enumerate(tqdm(grouper(batch_size, sliding_window(gt, step=stride, window_size=window_size)), total=total, leave=False)):\n","            # Display in progress results\n","                    \n","            # Build the tensor\n","            image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n","            image_patches = np.asarray(image_patches)\n","            image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n","            \n","            # Do the inference\n","            outs = net(image_patches)\n","            outs = F.softmax(outs, dim=1)\n","            outs = outs.data.cpu().numpy()\n","            \n","            # Fill in the results array\n","            for out, (x, y, w, h) in zip(outs, coords):\n","                out = out.transpose((1,2,0))\n","                pred[x:x+w, y:y+h] += out\n","            del(outs)\n","\n","        pred = np.argmax(pred, axis=-1)\n","        \n","        fig = plt.figure()\n","        fig.add_subplot(1,2,1)\n","        plt.imshow(np.asarray(255 * img[:,:,1], dtype='uint8'))\n","        fig.add_subplot(1,2,2)\n","        plt.imshow((pred))\n","        plt.show()\n","        return pred\n","                                 \n","      \n","net = UNetOriginal((IN_CHANNELS,WINDOW_SIZE[0],WINDOW_SIZE[1]))      \n","net.cuda()\n","\n","path = F\"/content/gdrive/My Drive/UNet_Glaciers\"\n","net.load_state_dict(torch.load(path))\n","\n","# We define the scheduler\n","base_lr = 0.01\n","optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.90, weight_decay=0.0005)\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [47,49], gamma=0.1)\n","\n","test_ids=[1]\n","test_files  = glob(r'/content/gdrive/My Drive/Landsat07image.tif')\n","print(test_files)\n","pred = testOnly(net, test_ids, all=False, stride=min(WINDOW_SIZE))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['/content/gdrive/My Drive/Landsat07image.tif']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mdy7ux3jw9dc","colab_type":"code","colab":{}},"source":["from folium import plugins\n","m=folium.Map(location=[ 36.07,73.691])\n","mapid=image.getMapId({'bands':['B5','B4','B3']})    \n","folium.TileLayer(\n","  tiles=EE_TILES.format(**mapid),\n","  attr='Google Earth Engine',\n","  overlay=True,\n"," ).add_to(m)  \n","#m.add_child(folium.LayerControl())\n","\n","\n","bounds1 = [[36.0,73.5],[37.0, 74.0]]\n","\n","img = folium.raster_layers.ImageOverlay(\n","  name='sg',\n","  image=pred,\n","  bounds=bounds1,\n","  interactive=True,\n",")\n","img.add_to(m)\n","m.add_child(folium.LayerControl())\n","\n","\n"],"execution_count":0,"outputs":[]}]}